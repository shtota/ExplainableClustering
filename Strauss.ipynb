{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import plotly.graph_objects as go\n",
    "from collections import defaultdict, Counter\n",
    "from itertools import permutations, cycle, islice, chain\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.sparse import *\n",
    "from scipy.stats import wasserstein_distance\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score, accuracy_score, recall_score, precision_score,mean_squared_error, f1_score\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, Birch, DBSCAN, OPTICS\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.linear_model import LogisticRegression as LR\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "#from interpret import show\n",
    "#from interpret.perf import ROC\n",
    "#from interpret.glassbox import ExplainableBoostingClassifier, LogisticRegression, ClassificationTree, DecisionListClassifier\n",
    "#from interpret.blackbox import ShapKernel\n",
    "#import shap\n",
    "#from xgboost import XGBClassifier\n",
    "#from interpret.data import ClassHistogram\n",
    "\n",
    "#import tensorflow as tf\n",
    "#import tensorboard as tb\n",
    "#tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "from data_loader import *\n",
    "from utils import *\n",
    "\n",
    "\n",
    "#from torch.utils.tensorboard import SummaryWriter\n",
    "sns.set()\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "Dataset()\n",
    "Users()\n",
    "CityStats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LOAD_EMBEDDINGS = True\n",
    "if LOAD_EMBEDDINGS and os.path.exists(EMBEDDING_PATH):\n",
    "    with open(EMBEDDING_PATH, 'rb') as f:\n",
    "        embeddings_dict = pickle.load(f)\n",
    "else:\n",
    "    dataset = [Dataset().id_to_transaction[t_id].to_sentence(barcodes) for t_id, barcodes in Dataset().barcode_iterator()]\n",
    "    print('created_dataset')\n",
    "    gensim_skipgram = Word2Vec(dataset,vector_size=300, window=100, min_count=1, workers=8,epochs=5,sample=1e-4, shrink_windows=False,) \n",
    "    with open(EMBEDDING_PATH, 'wb') as f:\n",
    "        pickle.dump(gensim_skipgram.wv, f)\n",
    "    embeddings_dict = gensim_skipgram.wv\n",
    "    del dataset\n",
    "\n",
    "products = sorted(Dataset().products, key=lambda x: x.usages, reverse=True)[:5000]\n",
    "product_vectors = np.vstack([embeddings_dict[x.barcode] for x in products])\n",
    "\n",
    "print(Dataset().barcode_to_product['7290004127329'].name)\n",
    "for x, y in embeddings_dict.most_similar('7290004127329'):\n",
    "    print(Dataset().barcode_to_product[x].name, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_show = 60\n",
    "decompose(product_vectors[:300], \n",
    "          show=to_show, \n",
    "          hue=[smart_inverse(x.hierarchy_names[2]) for x in products[:to_show]], \n",
    "          annotations=[smart_inverse(x.name) for x in products[:to_show]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# City stats block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_df = pd.DataFrame(CityStats().stats_df)\n",
    "stats_df = stats_df[stats_df.n_users > 30]\n",
    "clalit_cities = pd.read_csv('./stats/cities_clalit.csv')\n",
    "clalit_cities.columns=['city', 'district']\n",
    "clalit_cities.city = clalit_cities.city.map(lambda x: convert_city_name(x))\n",
    "city_to_region = {x:translate_district(y) for x,y in clalit_cities.values}\n",
    "stats_df['district'] = stats_df.index.map(lambda x: city_to_region.get(x, 'Unknown'))\n",
    "stats_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clalit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "districts = \"\"\"Jerusalem/1\n",
    "Tel Aviv/2\n",
    "Dan - Petach Tikva/4\n",
    "Haifa/5\n",
    "Central/7\n",
    "South/9\n",
    "Sharon - Shomron/12\n",
    "North/15\n",
    "Eilat/16\"\"\".split('\\n')\n",
    "districts = [x.split('/') for x in districts]\n",
    "districts = {x[1]: x[0] for x in districts}\n",
    "\n",
    "df = pd.read_csv('./stats/splits.csv')\n",
    "df = df[df.condition == 'active_malignancy=0 and ibd=0 and rhemartoid_arthritis=0']\n",
    "df = df[df.pop_name == 'overall']\n",
    "df=df[df.test_name != 'znb']\n",
    "\n",
    "clalit_splits = df[(df.first_split == 'sector') & (df.second_split == 'district')& (df.third_split == 'age_group')& (df.fourth_split.isna())]\n",
    "clalit_splits['district'] = clalit_splits.second_split_val.apply(lambda x: districts[x])\n",
    "clalit_splits=clalit_splits[clalit_splits.district != 'Eilat']\n",
    "districts =  clalit_splits.district.unique() \n",
    "\n",
    "clalit_splits['sector'] = clalit_splits.first_split_val.apply(lambda x: {'1': 'Arab', '2': \"Haredi\", '9': 'Others'}[x])\n",
    "clalit_splits = clalit_splits[clalit_splits.sector != 'Arab']\n",
    "clalit_splits['age'] = clalit_splits['third_split_val'].apply(lambda x: '09-13' if x== '9-13' else x)\n",
    "\n",
    "# Normal percentages for test/district/sector\n",
    "norms = {}\n",
    "for test in clalit_splits.test_name.unique():\n",
    "    for district in clalit_splits.district.unique():\n",
    "        for sector in clalit_splits.sector.unique():\n",
    "            cs = clalit_splits[(clalit_splits.test_name == test) & (clalit_splits.district == district) & (clalit_splits.sector == sector)].set_index('age').normal_pct.to_dict()\n",
    "            norms[(test,district, sector)] = cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clalit_splits = df[(df.first_split == 'sector') & (df.second_split == 'age_group')& (df.third_split.isna())]\n",
    "#clalit_splits['district'] = clalit_splits.second_split_val.apply(lambda x: districts[x])\n",
    "clalit_splits['sector'] = clalit_splits.first_split_val.apply(lambda x: {'1': 'Arab', '2': \"Haredi\", '9': 'Others'}[x])\n",
    "clalit_splits = clalit_splits[clalit_splits.sector != 'Arab']\n",
    "clalit_splits['age'] = clalit_splits['second_split_val'].apply(lambda x: '09-13' if x== '9-13' else x)\n",
    "# Normal percentages for test/sector\n",
    "for test in clalit_splits.test_name.unique():\n",
    "    for sector in clalit_splits.sector.unique():\n",
    "        cs = clalit_splits[(clalit_splits.test_name == test) & (clalit_splits.sector == sector)].set_index('age').normal_pct.to_dict()\n",
    "        norms[(test,sector)] = cs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ages = sorted(clalit_splits.age.unique())[:8]\n",
    "distances = {}\n",
    "city_list = []\n",
    "for row in stats_df.iterrows():\n",
    "    city, series = row[0], row[1]\n",
    "    if series.district == 'Unknown' or series.district == 'Eilat':\n",
    "        continue\n",
    "    city_list.append(city)\n",
    "    ages_v = series.values[2:10]\n",
    "    ages_v = ages_v/ages_v.sum()\n",
    "    orig_district = series.district\n",
    "    for test in clalit_splits.test_name.unique():\n",
    "        for district in districts:\n",
    "            distances[test, city, district] = 0\n",
    "            for i, age in enumerate(ages):\n",
    "                city_norm = norms[test,orig_district, 'Others'][age] if age in norms[test,orig_district, 'Others'] else norms[test, 'Others'][age]\n",
    "                new_region_norm = norms[test,district, 'Others'][age] if age in norms[test,district, 'Others'] else norms[test, 'Others'][age]\n",
    "                distances[test, city, district] += (1-series.haredim/100)*np.abs(city_norm - new_region_norm)*ages_v[i]\n",
    "                \n",
    "                city_norm = norms[test,orig_district, 'Haredi'][age] if age in norms[test,orig_district, 'Haredi'] else norms[test, 'Haredi'][age]\n",
    "                new_region_norm = norms[test,district, 'Haredi'][age] if age in norms[test,district, 'Haredi'] else norms[test, 'Haredi'][age]\n",
    "                distances[test, city, district] += series.haredim/100*np.abs(city_norm - new_region_norm)*ages_v[i]\n",
    "\n",
    "city_list = sorted(city_list, key=lambda x: city_to_region[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Food distance:\n",
    "### L2 distance between embeddings\n",
    "\n",
    "# Age distance:\n",
    "### Wasserstein distance, also known as the earth mover’s distance, since it can be seen as the minimum amount of “work” required to transform $u$ into $v$, where “work” is measured as the amount of distribution weight that must be moved, multiplied by the distance it has to be moved.\n",
    "\n",
    "# Health distance:\n",
    "\n",
    "### <b>Assume we can \"move\" city A from district A to district B, completely replacing all health statistics. How many people will change their health status according to test T? </b>\n",
    "#### $d(cityA,districtB) = \\sum_{test \\ T} \\sum_{ages \\ in \\ A} \\ (p^A_{age} \\cdot |normal_T A - normal_T B|)$\n",
    "\n",
    "\n",
    "### <b>Now imagine we swap districts of city A and B:</b>\n",
    "#### $d(cityA,cityB) = d(cityA,distictB) + d(cityB,districtA)$\n",
    "\n",
    "\n",
    "# Total Distance:\n",
    "### We scale distances of each type to have average ~1, and then take weighted sum: $d(A,B)=w_1 \\cdot d_{age} + w_2 \\cdot d_{food} + w_3 \\cdot d_{health}$\n",
    "\n",
    "### <b>Problem: citites in the same district receive distance = 0</b>\n",
    "#### Current solution: for cities in the same district we set heatlh distance as average between food and age distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "health_distances = np.zeros((len(city_list), len(city_list)))\n",
    "for i, c1 in enumerate(city_list):\n",
    "    d1 = city_to_region[c1]\n",
    "    \n",
    "    for j, c2 in enumerate(city_list):\n",
    "        d2 = city_to_region[c2]\n",
    "        if d1 == d2:\n",
    "            health_distances[i,j] = 0\n",
    "            continue\n",
    "        distance = 0\n",
    "        for test in clalit_splits.test_name.unique():\n",
    "            distance += distances[test, c1, d2]\n",
    "            distance += distances[test, c2, d1]\n",
    "        health_distances[i,j] = distance\n",
    "mean = health_distances.mean()\n",
    "health_distances /= mean\n",
    "health_distances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ages = CityStats().population_df.loc[city_list].values[:, 2:68].astype(float)\n",
    "totals = ages.sum(axis=1)\n",
    "\n",
    "ages = np.divide(ages.T,totals).T\n",
    "age_distances = pairwise_distances(ages, metric=wasserstein_distance)\n",
    "age_distances /= age_distances.mean()\n",
    "mean_ages = ages.dot(np.array(list(range(66))))\n",
    "\n",
    "city_embeddings = np.vstack([embeddings_dict[x] for x in city_list])\n",
    "city_embeddings_normalized = normalize(city_embeddings, axis=1)\n",
    "food_distances = 2-2*city_embeddings_normalized.dot(city_embeddings_normalized.T)\n",
    "\n",
    "food_distances *= (food_distances > 0)\n",
    "food_distances.shape, food_distances.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_heatmaps():\n",
    "    regions = sorted({city_to_region[x] for x in city_list})\n",
    "    health_distances_reg = np.zeros((len(city_list), len(regions)))\n",
    "    for i, c1 in enumerate(city_list):\n",
    "        for j, d2 in enumerate(regions):\n",
    "            d1 = city_to_region[c1]\n",
    "            if d1 == d2:\n",
    "                health_distances_reg[i,j] = 0\n",
    "                continue\n",
    "            distance = 0\n",
    "            for test in clalit_splits.test_name.unique():\n",
    "                distance += distances[test, c1, d2]\n",
    "            health_distances_reg[i,j] = distance\n",
    "    mean = health_distances_reg.mean()\n",
    "    health_distances_reg /= mean\n",
    "    plt.figure(figsize=(5,20))\n",
    "    df = pd.DataFrame(health_distances_reg.round(1), columns=regions, index=[smart_inverse(x) for x in city_list])\n",
    "    sns.heatmap(df, annot=True)\n",
    "    plt.savefig('./img/Health_regions.png')\n",
    "    plt.figure(figsize=(20,20))\n",
    "    df = pd.DataFrame(health_distances.round(1), columns=[smart_inverse(x) for x in city_list], index=[smart_inverse(x) for x in city_list])\n",
    "    p = sns.heatmap(df, annot=True)\n",
    "    p.set_title('Health')\n",
    "    plt.savefig('./img/health.png')\n",
    "    plt.figure(figsize=(20,20))\n",
    "    df = pd.DataFrame(age_distances.round(1), columns=[smart_inverse(x) for x in city_list], index=[smart_inverse(x) for x in city_list])\n",
    "    p = sns.heatmap(df, annot=True)\n",
    "    p.set_title('Age')\n",
    "    plt.savefig('./img/age.png')\n",
    "    plt.figure(figsize=(20,20))\n",
    "    df = pd.DataFrame(food_distances.round(1), columns=[smart_inverse(x) for x in city_list], index=[smart_inverse(x) for x in city_list])\n",
    "    p = sns.heatmap(df, annot=True)\n",
    "    p.set_title('Food')\n",
    "    plt.savefig('./img/food.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_distance(food_weight=1, age_weight=0, health_weight=2):\n",
    "    total_distance = np.zeros((len(city_list), len(city_list)))\n",
    "\n",
    "    for i, c1 in enumerate(city_list):\n",
    "        d1 = city_to_region[c1]\n",
    "        for j, c2 in enumerate(city_list):\n",
    "            d2 = city_to_region[c2]\n",
    "            if i == j:\n",
    "                total_distance[i,j] = 0\n",
    "            elif d1 == d2:\n",
    "                total_distance[i,j] = food_distances[i,j]*(food_weight+health_weight/2)+age_distances[i,j]*(age_weight+health_weight/2)\n",
    "            else:\n",
    "                total_distance[i,j] = food_distances[i,j]*food_weight+age_distances[i,j]*age_weight + health_distances[i,j]*health_weight\n",
    "    return total_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "p = decompose(get_distance(0,1,0), init='random', metric='precomputed', title=f'by age', hue=mean_ages, annotations=[smart_inverse(x) for x in city_list], perplexity=20, learning_rate=50, ee=20)\n",
    "for food, age, health in [(1,0,0),(1,0,1), (1,1,1), (1,1,5), (5,1,1)]:\n",
    "    total_distance = get_distance(food,age,health)\n",
    "    p = decompose(total_distance, init='random', metric='precomputed', title=f'by combination. Food importance: {food}. Age importance: {age}. Health importance: {health}. ', hue=stats_df.loc[city_list].district, annotations=[smart_inverse(x) for x in city_list], perplexity=20, learning_rate=50, ee=20)\n",
    "    p.figure.savefig(f'./img/Mixed map {food} {age} {health}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LOAD_VECS = True\n",
    "if LOAD_VECS and os.path.exists(CITY_PRODUCT_COUNTS_PATH):\n",
    "    with open(CITY_PRODUCT_COUNTS_PATH, 'rb') as f:\n",
    "        city_vectors = pickle.load(f)\n",
    "else:\n",
    "    city_vectors = {c: np.zeros(sum(Dataset().hierarchy_sizes)) for c in city_list}\n",
    "    city_set = set(city_list)\n",
    "    for t_id, barcodes in Dataset().barcode_iterator():\n",
    "        t = Dataset().id_to_transaction[t_id]\n",
    "        city = Users().active_users_cities.get(t.user_id, 'none')\n",
    "        if city not in city_set:\n",
    "            continue\n",
    "        #user_vectors[t.user_id][[Dataset().barcode_to_product[barcode].hierarchy_indices[3]-offset for barcode in t.barcodes]] += 1\n",
    "        for barcode in barcodes:\n",
    "            product = Dataset().barcode_to_product[barcode]\n",
    "            city_vectors[city][product.hierarchy_indices] += 1\n",
    "    with open(CITY_PRODUCT_COUNTS_PATH, 'wb') as f:\n",
    "        pickle.dump(city_vectors, f)\n",
    "\n",
    "index_to_name = {v:k for k,v in Dataset().index_of.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clusters = {}\n",
    "clusters['north'] = stats_df[stats_df.district == 'North'].index.values.tolist()\n",
    "clusters['south'] = stats_df[stats_df.district == 'South'].index.values.tolist()\n",
    "clusters['haifa'] = stats_df[stats_df.district == 'Haifa'].index.values.tolist()\n",
    "clusters['religion'] = stats_df[stats_df.haredim > 60 ].index.values.tolist()\n",
    "clusters['jerusalem'] = stats_df[(stats_df.district == 'Jerusalem') & (stats_df.haredim < 60)].index.values.tolist()\n",
    "clusters['jerusalem_and_religion'] =  stats_df[(stats_df.district == 'Jerusalem') | (stats_df.haredim > 60)].index.values.tolist()\n",
    "clusters['sharon'] = [x for x in stats_df[stats_df.district == 'Sharon - Shomron'].index.values.tolist() if x not in [\"רמת השרון\", \"כפר סבא\", \"רעננה\", \"הרצלייה\"]]\n",
    "#clusters['sharon'] = [x for x in sharon]\n",
    "#tel_aviv = [\"חולון\", \"בת ים\", \"ראשון לציון\"]\n",
    "clusters['center_dan'] = [x for x in city_list if x not in chain.from_iterable(clusters.values())]\n",
    "\n",
    "city_to_cluster = {}\n",
    "for k,v in clusters.items():\n",
    "    for city in v:\n",
    "        city_to_cluster[city] = k\n",
    "        \n",
    "for city in sorted(city_list):\n",
    "    clusters[city] = [city]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for food, age, health in [(3,0,1)]:\n",
    "    total_distance = get_distance(food,age,health)\n",
    "    p = decompose(total_distance, init='random', metric='precomputed', title=f'by combination. Food importance: {food}. Age importance: {age}. Health importance: {health}. ', hue=stats_df.loc[city_list].district, annotations=[smart_inverse(x) for x in city_list], perplexity=20, learning_rate=50, ee=50)\n",
    "    p = decompose(total_distance, init='random', metric='precomputed', title=f'by combination. Food importance: {food}. Age importance: {age}. Health importance: {health}. ', \n",
    "                  hue=[city_to_cluster[x] for x in city_list], annotations=[smart_inverse(x) for x in city_list], perplexity=20, learning_rate=50, ee=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ipywidgets import HBox, VBox, interact, Layout\n",
    "d = []\n",
    "cluster1 = widgets.Dropdown(options=list(clusters.keys()),value='jerusalem_and_religion',description='Cluster 1:',disabled=False)\n",
    "cluster2 = widgets.Dropdown(options=['not cluster 1'] + list(clusters.keys()) ,value='not cluster 1',description='Cluster 2:',disabled=False)\n",
    "level = widgets.Dropdown(options=['CLASS', 'CATEGORY', 'SUB-CATEGORY', 'BARCODE'] ,value='SUB-CATEGORY',description='Level:',disabled=False)\n",
    "number_1 = widgets.IntSlider(min=10, max=50, step=5, value=15, description='# features +')\n",
    "number_2 = widgets.IntSlider(min=0, max=30, step=5, value=15,description='# features -')\n",
    "\n",
    "food_weight = widgets.IntSlider(min=0, max=5, step=1, value=3,description='Food importance', style = {'description_width': 'initial'})\n",
    "age_weight = widgets.IntSlider(min=0, max=5, step=1, value=0,description='Age importance', style = {'description_width': 'initial'})\n",
    "health_weight = widgets.IntSlider(min=0, max=5, step=1, value=1,description='Health importance', style = {'description_width': 'initial'})\n",
    "\n",
    "ui_weights = HBox([food_weight, age_weight, health_weight])\n",
    "ui_clusters = HBox([cluster1, cluster2])\n",
    "ui_explanation = HBox([level, number_1, number_2])\n",
    "ui = VBox([ui_weights, ui_clusters, ui_explanation])\n",
    "\n",
    "transformed_X = None\n",
    "vizualization_weights = (0,0,0)\n",
    "\n",
    "def show_food_difference_embeddings(city_list1, city_list2, number_1, number_2):\n",
    "    vector1 = np.vstack([embeddings_dict[x] for x in city_list1]).mean(axis=0)\n",
    "    vector2 = np.vstack([embeddings_dict[x] for x in city_list2]).mean(axis=0)\n",
    "    difference_vector = vector1 - vector2\n",
    "    items = embeddings_dict.most_similar(difference_vector, topn=1000)\n",
    "    items = [(Dataset().barcode_to_product[x[0]].name, min(x[1], 1)) for x in items if x[0] in Dataset().barcode_to_product][:number_1]\n",
    "    neg_items = embeddings_dict.most_similar(-difference_vector, topn=1000)\n",
    "    items += [(Dataset().barcode_to_product[x[0]].name, -x[1]) for x in neg_items if x[0] in Dataset().barcode_to_product][:number_2]\n",
    "    items = [(smart_inverse(x[0]), x[1]) for x in items]\n",
    "    df = pd.DataFrame(items, columns=['name', 'similarity'])\n",
    "    plt.figure(figsize=(15, len(items)/3))\n",
    "    sns.barplot(x='similarity', y='name', data=df)\n",
    "    \n",
    "def on_update(cluster1, cluster2, level, number_1, number_2):\n",
    "    root = Path(f'./img/clusters/{cluster1} - {cluster2}/')\n",
    "    os.makedirs(root, exist_ok=True)\n",
    "    city_list1 = clusters[cluster1]\n",
    "    if len(city_list1) == 1:\n",
    "        cluster1 = cluster1[::-1]\n",
    "        \n",
    "    if cluster2 == 'not cluster 1':\n",
    "        city_list2 = [x for x in city_list if x not in city_list1]\n",
    "        cluster2 = f'not {cluster1}'\n",
    "    else:\n",
    "        city_list2 = clusters[cluster2]\n",
    "    \n",
    "    df1 = stats_df.loc[city_list1]\n",
    "    df2 = stats_df.loc[city_list2]\n",
    "    \n",
    "    text =  f'{cluster1}       |      {cluster2} \\n'\n",
    "    text += f'Population:       {df1.population.sum()//1000}K      |      {df2.population.sum()//1000}K \\n'\n",
    "    text += f'Users:           {df1.n_users.sum()}      |      {df2.n_users.sum()} \\n'\n",
    "    text += f'Socioeconomic: {(df1.socio*df1.n_users).sum()/df1.n_users.sum():.2f}      |      {(df2.socio*df2.n_users).sum()/df2.n_users.sum():.2f} \\n'\n",
    "    text += f'Periphery:    {(df1.periphery*df1.n_users).sum()/df1.n_users.sum():.2f}      |      {(df2.periphery*df2.n_users).sum()/df2.n_users.sum():.2f} \\n'\n",
    "    text += f'Avg salary:    {(df1.salary*df1.n_users).sum()/df1.n_users.sum():.0f}      |      {(df2.salary*df2.n_users).sum()/df2.n_users.sum():.0f} \\n'\n",
    "    f = plt.figure(figsize=(10,5))\n",
    "    ax = f.add_subplot(111)\n",
    "    plt.text(0.5,0.5,text,horizontalalignment='center', verticalalignment='center', transform = ax.transAxes, fontsize=20)\n",
    "    ax.grid(False)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    plt.savefig(root / '0_stats.png')\n",
    "    \n",
    "    # combined 2d map    \n",
    "    global transformed_X, vizualization_weights\n",
    "    if vizualization_weights != (food_weight.value,age_weight.value,health_weight.value): # checking if weights changed\n",
    "        vizualization_weights = (food_weight.value,age_weight.value,health_weight.value)\n",
    "        transformed_X = None\n",
    "        \n",
    "    hue = [cluster1 if city in city_list1 else (cluster2 if city in city_list2 else 'no cluster') for city in city_list]\n",
    "    p, transformed_X = decompose(get_distance(food_weight.value,age_weight.value,health_weight.value), init='random', metric='precomputed', \n",
    "                                 title=f'by combination. Food importance: {food_weight.value}. Age importance: {age_weight.value}. Health importance: {health_weight.value}. ', \n",
    "                                 hue=hue, hue_order=[cluster1, cluster2, 'no cluster'], \n",
    "                                 annotations=[smart_inverse(x) for x in city_list], \n",
    "                                 perplexity=20, learning_rate=50, ee=50, figsize=(15,10), \n",
    "                                 return_coordinates=True, transformed_X=transformed_X)\n",
    "    plt.savefig(root / '1_map.png')\n",
    "    \n",
    "    # food difference\n",
    "    level = ['CLASS', 'CATEGORY', 'SUB-CATEGORY', 'BARCODE'].index(level) + 1\n",
    "    start_index = sum(Dataset().hierarchy_sizes[:level])\n",
    "    end_index = start_index + Dataset().hierarchy_sizes[level]\n",
    "    vector1 = normalize(np.sum(np.vstack([city_vectors[x][start_index:end_index] for x in city_list1]),axis=0).reshape(1,-1), norm='l1')[0]\n",
    "    vector2 = normalize(np.sum(np.vstack([city_vectors[x][start_index:end_index] for x in city_list2]),axis=0).reshape(1,-1), norm='l1')[0]\n",
    "    diff = vector1-vector2\n",
    "    positive_idx = np.argpartition(diff, -number_1)[-number_1:]\n",
    "    items = [(smart_inverse(index_to_name[i+start_index]), diff[i]*100) for i in positive_idx]\n",
    "    negative_idx = np.argpartition(diff, number_2)[:number_2]\n",
    "    items += [(smart_inverse(index_to_name[i+start_index]), diff[i]*100) for i in negative_idx]\n",
    "    df = pd.DataFrame(items, columns=['name', 'frequency(%)']).sort_values('frequency(%)',ascending=False)\n",
    "    #df['name'] = ['item' + str(x) for x in range(len(df))]\n",
    "    plt.figure(figsize=(15, len(items)/3))\n",
    "    p = sns.barplot(x='frequency(%)', y='name', data=df)\n",
    "    p.set_title(f'Food frequency (right - {cluster1}')\n",
    "    plt.savefig(root / 'food_frequency.png')\n",
    "    \n",
    "    # Age distribution\n",
    "    plt.figure(figsize=(10,7))    \n",
    "    ages = CityStats().population_df.loc[city_list].values[:, 2:68].astype(float)\n",
    "    ages = np.divide(ages.T,ages.sum(axis=1)).T\n",
    "    index1 = [i for i, x in enumerate(city_list) if x in city_list1]\n",
    "    index2 = [i for i, x in enumerate(city_list) if x in city_list2]\n",
    "    df = pd.DataFrame(ages, columns=list(range(66)))\n",
    "    df['cluster'] = ['1' if x in city_list1 else '2' for x in city_list]\n",
    "    p=sns.lineplot(y=ages[index1].mean(axis=0), x=list(range(66)))\n",
    "    p=sns.lineplot(y=ages[index2].mean(axis=0), x=list(range(66)))\n",
    "    plt.legend([cluster1, cluster2])\n",
    "    plt.title('Age distribution')\n",
    "    plt.savefig(root / '3_age_distribution.png')\n",
    "    \n",
    "    # health differences\n",
    "    test_distances = defaultdict(int)\n",
    "    for test in clalit_splits.test_name.unique():\n",
    "        for i, c1 in enumerate(city_list1):\n",
    "            d1 = city_to_region[c1]\n",
    "            for j, c2 in enumerate(city_list2):\n",
    "                d2 = city_to_region[c2]\n",
    "                test_distances[test] += distances[test, c1, d2]\n",
    "                test_distances[test] += distances[test, c2, d1]\n",
    "    df = pd.DataFrame(test_distances.items(), columns=['test', 'difference']).sort_values('difference')\n",
    "    plt.figure(figsize=(10,5))\n",
    "    sns.barplot(x='difference', y='test', data=df)\n",
    "    plt.savefig(root / '4_health_1.png')\n",
    "    \n",
    "    ages = sorted(clalit_splits.age.unique())[:8]\n",
    "    f, ax = plt.subplots(2,2, figsize=(12,12))\n",
    "    for i, test in enumerate(df.test.values[-4:]):\n",
    "        data = defaultdict(int)\n",
    "        for city in city_list1 + city_list2:\n",
    "            district = city_to_region[city]\n",
    "            \n",
    "            religion_rate = stats_df.loc[city].haredim\n",
    "            population_percentage = stats_df.loc[city].population\n",
    "            population_percentage /= (stats_df.loc[city_list1].population.sum() if city in city_list1 else stats_df.loc[city_list2].population.sum())\n",
    "            for age in ages:\n",
    "                city_norm = (norms[test,district, 'Others'][age] if age in norms[test,district, 'Others'] else norms[test, 'Others'][age])*(1-religion_rate/100)\n",
    "                city_norm += (norms[test,district, 'Haredi'][age] if age in norms[test,district, 'Haredi'] else norms[test, 'Haredi'][age])*religion_rate/100\n",
    "                data[age, [cluster1, cluster2][city in city_list1]] += city_norm*population_percentage\n",
    "        df = pd.DataFrame([list(k)+[v] for k,v in data.items()], columns=['age', 'cluster', 'normal percentage'])\n",
    "        p = sns.lineplot(x='age', y='normal percentage', hue='cluster', hue_order=[cluster1, cluster2], ax=ax[i//2][i%2], data=df)\n",
    "        p.set_title(test)\n",
    "    plt.savefig(root / '4_health_2.png')\n",
    "    \n",
    "    \n",
    "#interact(on_button_clicked,cluster1=cluster1, cluster2=cluster2);\n",
    "out = widgets.interactive_output(on_update, {'cluster1': cluster1, 'cluster2': cluster2, 'level': level, 'number_1': number_1, 'number_2': number_2})\n",
    "\n",
    "display(ui, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO:\n",
    "1) city-level clusters (text box or something)\n",
    "2) add category food graph (religion: + barcode food graph)\n",
    "3) prepare 8 dashboard screenshots (one for each cluster)\n",
    "4) Check exactly how socio and periphery value\n",
    "5) Add health distribution graph: top-4 tests with distribution curves (one grapoh for each test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MIN_DIFF = 1\n",
    "def _region_pairwise_rank():\n",
    "    for r1, v1 in ranks.items():\n",
    "        for r2, v2 in ranks.items():\n",
    "            vec1 = city_embeddings[[x==r1 for x in region_list]].mean(axis=0)\n",
    "            vec2 = city_embeddings[[x==r2 for x in region_list]].mean(axis=0)\n",
    "            comparison_X.append(vec1-vec2)\n",
    "            if np.abs(v1-v2) < MIN_DIFF:\n",
    "                comparison_y.append(0)\n",
    "                comparison_X.append(vec1-vec2)\n",
    "                comparison_y.append(1)\n",
    "                continue\n",
    "            comparison_y.append(v1 < v2)\n",
    "def fit(comparison_X, comparison_y):\n",
    "    model = LR(C=0.01, max_iter=10000)\n",
    "    model.fit(comparison_X, comparison_y)\n",
    "    top_products = [x.barcode for x in Dataset().products if x.usages > 10000]\n",
    "    return [[Dataset().barcode_to_product[x].name, x, (embeddings_dict[x].dot(model.coef_[0]) - model.intercept_)[0]] for x in top_products]\n",
    "    \n",
    "def importance_from_order(order, name):\n",
    "    order = order.split(' ')\n",
    "    ranks = {}\n",
    "    for region in set(user_regions):\n",
    "        rank = [4-i for i,x in enumerate(order) if x.find(region.split(' ')[0]) != -1][0]\n",
    "        ranks[region] = rank\n",
    "    comparison_X = []\n",
    "    comparison_y = []\n",
    "    for i, city1 in enumerate(city_list):\n",
    "        for j, city2 in enumerate(city_list):\n",
    "            if city1 == city2:\n",
    "                continue\n",
    "            vec1 = city_embeddings[i]\n",
    "            vec2 = city_embeddings[j]\n",
    "            comparison_X.append(vec1-vec2)\n",
    "            v1, v2 = ranks[city_to_region[city1]], ranks[city_to_region[city2]]\n",
    "\n",
    "            if np.abs(v1-v2) < MIN_DIFF:\n",
    "                comparison_y.append(0)\n",
    "                comparison_X.append(vec1-vec2)\n",
    "                comparison_y.append(1)\n",
    "                continue\n",
    "            comparison_y.append(v1 < v2)\n",
    "    data = fit(comparison_X, comparison_y)\n",
    "    column_name = f'{name} effect'\n",
    "    \n",
    "    df = pd.DataFrame(data=data, columns=['item', 'barcode', column_name])\n",
    "    df['name'] = df.item.map(smart_inverse)\n",
    "    df = df.sort_values(column_name)\n",
    "    f = plt.figure(figsize=(10,7))\n",
    "    p = sns.barplot(y='name', x=column_name, data=df.head(10).append(df.tail(10)))\n",
    "order = \"Jerusalem South Haifa|North Dan|Sharon|Central Tel\"\n",
    "importance_from_order(order, 'trig')\n",
    "\n",
    "order = 'Jerusalem Sharon|South|North|Haifa Dan|Central Tel'\n",
    "importance_from_order(order, 'hdl')\n",
    "\n",
    "order = 'South Jerusalem|Sharon|North Dan|Central Tel|Haifa'\n",
    "importance_from_order(order, 'transf')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempts to predict socio/religion from sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.2)\n",
    "\n",
    "city_embeddings = np.vstack([embeddings_dict[x] for x in stats_df.index.values])\n",
    "city_embeddings_normalized = normalize(city_embeddings, axis=1)\n",
    "for column in ['socio', 'haredim']:\n",
    "    model.fit(city_embeddings_normalized, stats_df[column].values)\n",
    "    top_products = [x.barcode for x in Dataset().products if x.usages > 10000]\n",
    "    column_name = column+' effect'\n",
    "    socio_df = pd.DataFrame(data=[[Dataset().barcode_to_product[x].name, x, model.predict(embeddings_dict[x].reshape(1, -1))[0] - model.intercept_] for x in top_products], columns=['item', 'barcode', column_name])\n",
    "    \n",
    "    socio_df['name'] = socio_df.item.map(smart_inverse)\n",
    "    socio_df = socio_df.sort_values(column_name)\n",
    "    f = plt.figure(figsize=(10,7))\n",
    "    p = sns.barplot(y='name', x=column_name, data=socio_df.head(10).append(socio_df.tail(10)))\n",
    "    p.set_title('Estimated ' + column +' correlation')\n",
    "    print(column, 'R squared',model.score(city_embeddings_normalized, stats_df[column].values), 'MAE',   mean_squared_error(model.predict(city_embeddings_normalized), stats_df[column].values, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embeddings = np.vstack([embeddings_dict[x] for x in Users().active_users])\n",
    "user_embeddings_normalized = normalize(user_embeddings, axis=1)\n",
    "model.fit(city_embeddings, stats_df['socio'].values)\n",
    "user_socio = model.predict(user_embeddings)\n",
    "model.fit(city_embeddings, stats_df['haredim'].values)\n",
    "user_religion = model.predict(user_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.2)\n",
    "model.fit(city_embeddings, stats_df['socio'].values)\n",
    "example_cities = list(stats_df.sort_values('n_users').index.values[-20:])\n",
    "example_city_socio = stats_df.loc[example_cities]['socio'].values\n",
    "city_users = [[x, smart_inverse(y), model.predict(embeddings_dict[x].reshape(1,-1))[0]]  for x,y in Users().active_users_cities.items() if y in example_cities]\n",
    "df = pd.DataFrame(city_users, columns=['u_id', 'city', 'predicted_socio'])\n",
    "plt.figure(figsize=(20,10))\n",
    "p = sns.violinplot(x='city', y='predicted_socio', data=df, order=df.groupby('city')['predicted_socio'].mean().sort_values().index)\n",
    "#for i in range(20):\n",
    "    #p.plot([i-0.5,i+0.5], [v[i], v[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = Ridge(alpha=0.2)\n",
    "model.fit(city_embeddings, stats_df['socio'].values)\n",
    "example_cities = list(stats_df.sort_values('n_users').index.values[-50:-40])+ list(stats_df.sort_values('n_users').index.values[-10:])\n",
    "example_city_socio = stats_df.loc[example_cities]['socio'].values\n",
    "city_users = [[x, smart_inverse(y), model.predict(embeddings_dict[x].reshape(1,-1))[0]]  for x,y in Users().active_users_cities.items() if y in example_cities]\n",
    "df = pd.DataFrame(city_users, columns=['u_id', 'city', 'predicted_socio'])\n",
    "plt.figure(figsize=(20,10))\n",
    "p = sns.violinplot(x='city', y='predicted_socio', data=df, order=[smart_inverse(x) for x in example_cities])\n",
    "for i in range(20):\n",
    "    p.plot([i-0.5,i+0.5], [example_city_socio[i], example_city_socio[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User clustering and explaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz_silhoette(X, cluster_labels, title='', ax=None):\n",
    "    if X.shape[0] > 10000:\n",
    "        idx = np.random.choice(X.shape[0], size=10000, replace=False)\n",
    "        X = X[idx]\n",
    "        cluster_labels = cluster_labels[idx]\n",
    "    \n",
    "    n_clusters = len(set(cluster_labels))\n",
    "    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n",
    "    \n",
    "    if ax is None:\n",
    "        f, ax1 = plt.subplots(1,1, figsize=(12,7))\n",
    "    else:\n",
    "        ax1 = ax\n",
    "    ax1.set_ylim([0, X.shape[0] + (n_clusters + 1) * 10])\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        # Aggregate the silhouette scores for samples belonging to\n",
    "        # cluster i, and sort them\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                          0, ith_cluster_silhouette_values,\n",
    "                          facecolor=color, edgecolor=color, alpha=0.7)\n",
    "\n",
    "        # Label the silhouette plots with their cluster numbers at the middle\n",
    "        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "        # Compute the new y_lower for next plot\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(title)\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values. mean=\" +str(np.mean(sample_silhouette_values)))\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    # The vertical line for average silhouette score of all the values\n",
    "    ax1.axvline(x=np.mean(sample_silhouette_values), color=\"red\", linestyle=\"--\")\n",
    "\n",
    "    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n",
    "    \n",
    "    if ax is None:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "clf = MiniBatchKMeans(n_clusters=9, random_state=4)\n",
    "#clf = GaussianMixture(n_components=5, random_state=3)\n",
    "user_clusters = clf.fit_predict(user_embeddings_normalized)\n",
    "viz_silhoette(user_embeddings_normalized, user_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for k in range(9,11,2):\n",
    "    for restart in range(5):\n",
    "        clf = MiniBatchKMeans(n_clusters=k, random_state=restart)\n",
    "    #clf = GaussianMixture(n_components=10, random_state=3)\n",
    "        y = clf.fit_predict(user_embeddings_normalized)\n",
    "        res.append([k, silhouette_score(user_embeddings_normalized, y)])\n",
    "sns.scatterplot(x='n_clusters', y='silhoette', data=pd.DataFrame(data=res, columns=['n_clusters', 'silhoette']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "LOAD_VECS = True\n",
    "if LOAD_VECS:\n",
    "    with open(CLUSTERING_PATH, 'rb') as f:\n",
    "        user_vectors = pickle.load(f)\n",
    "else:\n",
    "    #user_vectors = {u: np.zeros(sum(LEVEL_SIZE[:-1])) for u in {x.user_id for x in transactions}}\n",
    "    user_vectors = {u: np.zeros(Dataset().hierarchy_sizes[-2]) for u in Users().active_users}\n",
    "    offset = sum(Dataset().hierarchy_sizes[:-2])\n",
    "    for t in Dataset().transactions:\n",
    "        if t.user_id not in user_vectors.keys():\n",
    "            continue\n",
    "        user_vectors[t.user_id][[Dataset().barcode_to_product[barcode].hierarchy_indices[3]-offset for barcode in t.barcodes]] += 1\n",
    "        #user_vectors[t.user_id][p.hierarchy_indices[:min(p._representation+1, 4)]] += 1\n",
    "    with open(CLUSTERING_PATH, 'wb') as f:\n",
    "        pickle.dump(user_vectors, f)\n",
    "\n",
    "tf_matrix = np.vstack([user_vectors[x] for x in Users().active_users])  \n",
    "tf_normalized = normalize(tf_matrix, axis=1, norm='l2')\n",
    "\n",
    "del user_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l3 = sorted([x for x in Dataset().index_of.keys() if x.startswith('3_')])\n",
    "feature_names = [x[2:].replace('[', '(').replace(']', ')').replace('<', ' ') for x in l3]\n",
    "df = pd.DataFrame(data=tf_normalized, columns=feature_names)\n",
    "#\n",
    "\n",
    "from sklearn.utils import resample\n",
    "def split(cluster, np_array=False):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, [z==cluster for z in user_clusters], test_size=0.20, random_state=1)\n",
    "    if X_train.shape[0] > sum(y_train)*2:\n",
    "        z_df = X_train[[not x for x in y_train]]\n",
    "        z_df = resample(z_df, n_samples=sum(y_train)*2, random_state=1)\n",
    "        X_train = X_train[y_train].append(z_df)\n",
    "        y_train = np.array([1]*(len(X_train) - len(z_df)) +[0]*len(z_df))\n",
    "    # for the tests\n",
    "    #X_train, _, y_train, _ = train_test_split(X_train, y_train, test_size=0.8, random_state=1)\n",
    "    if np_array:\n",
    "        return X_train.values, X_test.values, y_train, y_test\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def viz_lr_mixed(cluster=0, k=20, positive=12):\n",
    "    X_train, X_test, y_train, y_test = split(cluster, True)\n",
    "    model = LR(max_iter=10000, penalty='l2', solver='liblinear', class_weight='balanced', C=100)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    f1 = recall_score(y_test, predictions)\n",
    "    positive_weights = X_train[y_train].mean(axis=0)*model.coef_[0]\n",
    "    negative_weights = X_train[~y_train].mean(axis=0)*model.coef_[0]\n",
    "    tmp = pd.DataFrame(sorted(list(zip(positive_weights, feature_names)), key=lambda x: x[0], reverse=True)[:positive], columns=['Logistic regression weight', 'sub-category'])\n",
    "    tmp2 = pd.DataFrame(sorted(list(zip(negative_weights, feature_names)), key=lambda x: x[0], reverse=True)[-(k-positive):], columns=['Logistic regression weight', 'sub-category'])\n",
    "    tmp = tmp.append(tmp2)\n",
    "    tmp['sub-category'] = tmp['sub-category'].map(smart_inverse)\n",
    "    f = plt.figure(figsize=(10,7))\n",
    "    p = sns.barplot(y='sub-category', x='Logistic regression weight', data=tmp)\n",
    "    \n",
    "    socio = user_socio[y_train].mean()\n",
    "    rel = user_religion[y_train].mean()\n",
    "    p.set_title('Cluster {} with {:.1f}% of users. Recall: {:.1f}%. Socio status: {:.2f}. Predicted haredim rate: {:.2f}'.format(cluster, sum(y_train)/len(df)*100,f1*100, round(socio, 2), round(rel, 2)))\n",
    "    plt.savefig('./img/'+str(cluster)+'.png')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(9):\n",
    "    viz_lr_mixed(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def viz_explanation(cluster=0):\n",
    "    X_train, X_test, y_train, y_test = split(cluster)\n",
    "    hist = ClassHistogram().explain_data(X_train, y_train, name = 'Train Data')\n",
    "\n",
    "    ebm = ExplainableBoostingClassifier(random_state=1, n_jobs=8, interactions=0, max_bins=30)\n",
    "    ebm.fit(X_train, y_train)   #Works on dataframes and numpy arrays\n",
    "    ebm_global = ebm.explain_global(name='EBM')\n",
    "    ebm_local = ebm.explain_local(X_test[y_test][:5].append(X_test[y_test].mean(), ignore_index=True), [1]*6, name='EBM') #first 5 positive instances and cluster center as 6th one\n",
    "    ebm_perf = ROC(ebm.predict_proba).explain_perf(X_test, y_test, name='EBM')\n",
    "    lr = LogisticRegression(random_state=1, feature_names=feature_names, penalty='l1', solver='liblinear', C=1)\n",
    "    lr.fit(X_train, y_train)\n",
    "    #sorted(zip(lr.sk_model_.coef_[0], feature_names), key=lambda x: x[0], reverse=True)[:10]\n",
    "    lr_perf = ROC(lr.predict_proba).explain_perf(X_test, y_test, name='Logistic Regression')\n",
    "\n",
    "    lr_global = lr.explain_global(name='Logistic Regression')\n",
    "    lr_local = lr.explain_local(X_test[y_test][:5].append(X_test[y_test].mean(), ignore_index=True), [1]*6, name='LR local' )\n",
    "    \n",
    "    show([hist, lr_global, lr_perf, ebm_global, ebm_perf, ebm_local, lr_local], share_tables=True)\n",
    "\n",
    "def viz_shap(cluster=0):\n",
    "    X_train, X_test, y_train, y_test = split(cluster, True)\n",
    "    model = XGBClassifier(use_label_encoder=False) \n",
    "    #model = sklearn.linear_model.LogisticRegression(max_iter=100, penalty='l1', solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(cluster, accuracy_score(y_test, predictions), recall_score(y_test, predictions))\n",
    "    X_train = pd.DataFrame(X_train)\n",
    "    X_train.columns = [x[::-1] for x in feature_names] if DATASET == 'strauss' else feature_names\n",
    "    explainer = shap.Explainer(model, X_train)\n",
    "    shap.initjs()\n",
    "    shap_values = explainer(X_train)\n",
    "    shap.summary_plot(shap_values, X_train)\n",
    "    \n",
    "def viz_lr(cluster=0, viz_center=False, k=20, positive=15):\n",
    "    X_train, X_test, y_train, y_test = split(cluster, True)\n",
    "    model = LR(max_iter=100, penalty='l1', solver='liblinear')\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "    print(accuracy_score(y_test, predictions), recall_score(y_test,predictions), precision_score(y_test, predictions))\n",
    "    if viz_center:\n",
    "        weights = X_train[y_train].mean(axis=0)*model.coef_[0]\n",
    "    else:\n",
    "        weights = model.coef_[0]\n",
    "    named_coef = list(zip(weights, feature_names))\n",
    "    tmp = pd.DataFrame(sorted(named_coef, key=lambda x: x[0], reverse=True), columns=['Logistic regression weight', 'sub-category'])\n",
    "    return tmp.head(positive).append(tmp.tail(k-positive))\n",
    "\n",
    "def create_lists(cluster=0):\n",
    "    X_train, X_test, y_train, y_test = split(cluster)\n",
    "    cluster_center = X_train.iloc[y_train].mean(axis=0)\n",
    "    \n",
    "    #ebm = ExplainableBoostingClassifier(random_state=1, n_jobs=8, interactions=0, max_bins=30)\n",
    "    #ebm.fit(X_train, y_train)\n",
    "    #ebm_list = []\n",
    "    \n",
    "    model = XGBClassifier(use_label_encoder=False) \n",
    "    model.fit(X_train, y_train)\n",
    "    explainer = shap.TreeExplainer(model, X_train)\n",
    "    shap_values = explainer(X_train.append(cluster_center, ignore_index=True))\n",
    "    shap_aggregated = shap_values[:-1,:][y_train,:].mean(axis=0).values\n",
    "    \n",
    "    shap_center = shap_values[-1,:]\n",
    "    \n",
    "    lr = LR(max_iter=100, penalty='l1', solver='liblinear')\n",
    "    lr.fit(X_train, y_train)\n",
    "    lr_prediction = X_train.values[y_train].mean(axis=0)*lr.coef_[0]\n",
    "    lr_weights = lr.coef_[0]\n",
    "    \n",
    "    return shap_aggregated, shap_center.values, lr_prediction, lr_weights\n",
    "\n",
    "def rank_to_score(rank, l):\n",
    "    return len(l)-rank-1\n",
    "\n",
    "def aggregate_lists(lists, method='rank'):\n",
    "    if method == 'score':\n",
    "        stacked_scores = np.stack(lists).T\n",
    "        norm_scores = sklearn.preprocessing.StandardScaler().fit_transform(stacked_scores)\n",
    "        new_scores = norm_scores.mean(1)\n",
    "        return new_scores\n",
    "    \n",
    "    scores = np.zeros(len(lists[0]))\n",
    "    for l in lists:\n",
    "        list_scores = Counter(np.abs(l))\n",
    "        less_than = {sc: sum([v for k,v in list_scores.items() if k < sc ]) for sc in list_scores.keys()}\n",
    "        for i, v in enumerate(np.abs(l)):\n",
    "            scores[i] += less_than[v]\n",
    "            \n",
    "    for i in range(len(lists[0])):\n",
    "        if all(l[i] >= 0 for l in lists):\n",
    "            continue\n",
    "        elif all(l[i] <= 0 for l in lists):\n",
    "            scores[i] = -scores[i]\n",
    "        else:\n",
    "            scores[i]=0\n",
    "    return scores\n",
    "\n",
    "def top_k_features(scores, k=20, positive=15):\n",
    "    best = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)\n",
    "    best = [(feature_names[i], scores[i]) for i in best]\n",
    "    return best[:positive] + best[-(k-positive):]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
